#!/usr/bin/env python2
"""
This is a utility to recursively search for files by name in a filesystem,
also looking inside archives to an arbitary depth.

While it's usually bad form to create a single monolithic file like this, it's
intended to be used as a single script without some complicated installation
procedure.
"""

import argparse
from collections import deque
from contextlib import closing, contextmanager
from cStringIO import StringIO
from fnmatch import fnmatchcase
import json
from os import walk
import os.path
import posixpath
import sys
from tarfile import TarFile
from zipfile import ZipFile

import magic

# Help text for command-line invocation
PROG_TEXT = (
"Recursively search for files by name, including inside certain kinds of "
"archives. Supported archives include .ZIP, .TAR, .TAR.GZ and .TAR.BZ2"
)

# Leading text for results
RESULT_INDENT = "  > "

# By default, the tarfile module will decode entries in a PAX-format archive and
# then re-encode them based on the current system. It would be better if we
# could just get the unicode sequence in the first place, but since we can't
# we'll just force it to re-encode to UTF-8 instead, and then decode it again
# when we need to.
TARFILE_ENCODING = 'utf8'

# Mystery! There is no official zipfile encoding, and the archive format doesn't
# contain it. The Python docs say that WinZip guesses CP-437, so let's use that!
ZIPFILE_ENCODING = 'cp437'

# What encoding should we use to interpret command line arguments? Who knows! If
# we get bytes off the command line, it could be sys.stdin.encoding. Or hey,
# maybe it's locale.getpreferredencoding()! Although if we're invoked from
# another program, they could pick anything and we wouldn't know. So let's just
# say it's this.
CLI_ENCODING = 'utf8'

# How many bytes to read to determine the type of file in an archive
MAGIC_BYTES_N = 1024

# Mime types for archives we can search inside
TAR_MIME_TYPES = {
    'application/x-bzip2' : 'r:bz2',
    'application/x-gzip'  : 'r:gz',
    'application/x-tar'   : 'r:'
}

ZIP_MIME_TYPES = ('application/zip', )

# This script follows the philosohpy of "encode data coming in, do internal
# processing on unicode sequence objects, encode data going out". The following
# are decoding/encoding utility functions.

def d_cl(some_bytes):
    """ See snarky note for CLI_ENCODING. """
    return some_bytes.decode(CLI_ENCODING)


def d_fs(some_bytes):
    """ At least when the string comes from the filesystem we know what to do...
    """
    return some_bytes.decode(sys.getfilesystemencoding())


def d_tr(some_bytes):
    """ And here, we can take complete control! """
    return some_bytes.decode(TARFILE_ENCODING)


class Path(object):
    """ Depending on whether we're searching or presenting results, we'll need
    to access both the split and joined forms of filesystem/archive paths. This
    class does the conversion once and keeps both forms so we don't keep
    converting back and forth.
    """
    
    @classmethod
    def os(cls, path):
        """ Split a path obtained from OS-related functions. Expects a unicode
        sequence. """
        normed = os.path.normpath(path)
        return cls(os.path.split(normed), normed)
    
    @classmethod
    def posix(cls, path):
        """ Splits a path that's known to be in POSIX '/'-separated form.
        Expects a unicode sequence. """
        normed = posixpath.normpath(path)
        return cls(posixpath.split(normed), normed)
    
    def __init__(self, paths, joined):
        """ Stores both the split and joined forms of a path. Expects a unicode
        sequence. """
        self.paths = paths
        self.joined = joined
        
    def __getitem__(self, key):
        """ Indexing works on the sequence of path components. """
        return self.paths[key]


# The NamedNode, DirectoryWalker and ArchiveWalker objects all have a "contents"
# member that is iterated over to list ALL files contained within them. The
# exact behaviour of this varies between them.

class NamedNode(object):
    """ Represents a file, whether a normal file or directory or one nested
    inside an archive. The file is denoted by a series of Path objects.
    
    Files that are directly in the file system have a single entry. Files that
    are nested inside archives have multiple paths (the first is the archive,
    the second is a nested archive, the (N-1)th is the archive containing the
    file, the Nth is the file itself).
    
    NamedNode objects have an empty "contents" member.
    """
    
    @classmethod
    def single(cls, path):
        """ Create a NamedNode from a single Path object. """
        return cls((path,))
    
    def __init__(self, paths):
        """ Create a NamedNode from a sequence of Path objects. """
        self.paths = paths
        self.contents = ()


class DirectoryWalker(object):
    """ Recurses through a filesystem starting at a given directory, looking for
    files in the filesystem itself and any archives it encounters.    
    """
    
    def __init__(self, path):
        """ Creates the directory walker starting at the given directory. The
        list of found files/directories/etc is not populated immediately, but is
        generated dynamically as the "contents" member is iterated over.
        """
        self.path = path
        self.paths = (path,)

    @property
    def contents(self):
        """ The contents of this generator are the files found by walking
        through the filesystem and inspecting any archives found. """
        # Walk through the filesystem, yielding all files and directories found
        for dirpath, dirnames, filenames in walk(self.path.joined):
            # For every file, we return both the file itself as a result, but
            # also inspect it to see if it's an archive.
            for fname in filenames:
                fpath = Path.os(d_fs(os.path.join(dirpath, fname)))
                
                # The file itself
                yield NamedNode.single(fpath)
                
                # Inspect it to see if it's an archive
                mime_type = magic.from_file(fpath.joined, mime=True)
                archive_mode = TAR_MIME_TYPES.get(mime_type, None)
                zipfile = mime_type in ZIP_MIME_TYPES
                
                if archive_mode:
                    # Generate the archive contents too
                    yield ArchiveWalker.from_path(fpath, archive_mode)
                elif zipfile:
                    # Generate the zipfile contents too
                    yield ZipWalker.from_path(fpath)
            
            # The directories don't need special treatment
            for dname in dirnames:
                yield NamedNode.single(Path.os(d_fs(os.path.join(dirpath, dname))))


class ArchiveWalker(object):
    """ Recursively searches the given archive, listing the contents and
    inspecting any nested archives for more files. Note that unlike the
    DirectoryWalker, an ArchiveWalker will populate its results as soon as it is
    created (ie. it does not act as a generator).
    """
    
    @classmethod
    def from_path(cls, path, *args, **kargs):
        """ Takes a Path object denoting an archive in the filesystem, and
        recursively lists the files inside (including any inside nested
        archives).
        """
        with TarFile.open(
                path.joined, *args,
                encoding=TARFILE_ENCODING, **kargs
                ) as tfobj:
            return cls(tfobj, (path,))
     
    @classmethod
    def from_buffer(cls, buf, paths, mode):
        """ Takes an already open buffer of archive data, and recursively lists
        the files inside (including any inside nested archives). The caller is
        responsible for closing the buffer. The buffer's position will be
        changed in this function.
        """
        buf.seek(0)
        with TarFile.open(
                fileobj=buf, mode=mode,
                encoding=TARFILE_ENCODING
                ) as tfobj:
            return cls(tfobj, paths)
    
    def __init__(self, tarfile, paths):
        """ Given an already open archive (a tarfile.TarFile object),
        recursively list the contents, including files inside archives contained
        in this one. The "paths" argument specifies the location of this archive
        in the filesystem or nested inside other archives. It is the caller's
        responsibility to close the archive.
        """
        self.tf = tarfile
        self.paths = paths
        self.contents = deque()
        self._populate()
        
    def _populate(self):
        """ Populates the "contents" member with the recursively found files. It
        is better to do it all at once so the file is not kept open for too
        long.
        """
        tfcontents = self.tf.getmembers()
        
        # This is a similar approach to that in DirectoryWalker, but the
        # getmembers() function returns a flat list rather than the recursive
        # structure of os.walk. We list each file and directory, and if a file
        # is an archive, we get access to the data as a buffer and recurse into
        # it.
        for member in tfcontents:
            member_path = Path.posix(member.name.decode(TARFILE_ENCODING))
            # This keeps track of our location in the filesystem and any
            # archives we're already nested inside.
            nested_paths = self.paths + (member_path,)
            self.contents.append(NamedNode(nested_paths))
            
            # Inspect files to see if they're archives
            if member.isfile():
                with closing(self.tf.extractfile(member)) as inner_file:
                    magic_bytes = inner_file.read(MAGIC_BYTES_N)
                    inner_mime_type = magic.from_buffer(
                        magic_bytes,
                        mime=True
                    )
                    
                    archive_mode = TAR_MIME_TYPES.get(
                        inner_mime_type,
                        None
                    )
                    
                    zipfile = inner_mime_type in ZIP_MIME_TYPES

                    # If it's an archive, create another ArchiveWalker instance
                    # from the extracted buffer
                    if archive_mode:
                        self.contents.extend(
                            ArchiveWalker.from_buffer(
                                inner_file,
                                nested_paths,
                                archive_mode
                            ).contents)
                    elif zipfile:
                        self.contents.extend(
                            ZipWalker.from_buffer(
                                inner_file,
                                nested_paths
                            ).contents)


class ZipWalker(object):
    """ Recursively searches the given zipfile, listing the contents and
    inspecting any nested archives for more files. Like the ArchiveWalker, a
    ZipWalker will populate its results as soon as it is created (ie. it does
    not act as a generator).
    """

    @classmethod
    def from_path(cls, path):
        """ Takes a Path object denoting a zipfile in the filesystem, and
        recursively lists the files inside (including any inside nested
        archives).
        """
        with ZipFile(path.joined, 'r') as zfobj:
            return cls(zfobj, (path,))
        
    @classmethod
    def from_buffer(cls, buf, paths):
        """ Takes an already open buffer of zipfile data, and recursively lists
        the files inside (including any inside nested archives). The caller is
        responsible for closing the buffer. The buffer's position will be
        changed in this function.
        """
        buf.seek(0)
        with ZipFile(buf, 'r') as zfobj:
            return cls(zfobj, paths)

    def __init__(self, zipfile, paths):
        """ Given an already open archive (a zipfile.ZipFile object),
        recursively list the contents, including files inside archives contained
        in this one. The "paths" argument specifies the location of this archive
        in the filesystem or nested inside other archives. It is the caller's
        responsibility to close the archive.
        """
        self.zf = zipfile
        self.paths = paths
        self.contents = deque()
        self._populate()

    def _populate(self):
        """ Populates the "contents" member with the recursively found files. It
        is better to do it all at once so the file is not kept open for too
        long.
        """
        zfcontents = self.zf.infolist()
        
        for member in zfcontents:
            # Although it's not documented, ZIP file paths are always '/'
            # separated, and directories end in a '/'
            member_path = Path.posix(member.filename.decode(ZIPFILE_ENCODING))
            nested_paths = self.paths + (member_path,)
            self.contents.append(NamedNode(nested_paths))
            
            # Files won't have a trailing '/'
            if not member.filename.endswith('/'):
                # Unfortunately, ZipFile extraction is not as seamless as it is
                # for TarFile objects. We need to read the file into memory and
                # operate on that buffer.
                with closing(StringIO(self.zf.read(member))) as inner_file:
                    magic_bytes = inner_file.read(MAGIC_BYTES_N)
                    inner_mime_type = magic.from_buffer(
                        magic_bytes,
                        mime=True
                    )
                    
                    archive_mode = TAR_MIME_TYPES.get(
                        inner_mime_type,
                        None
                    )
                    
                    zipfile = inner_mime_type in ZIP_MIME_TYPES
                    
                    if archive_mode:
                        self.contents.extend(
                            ArchiveWalker.from_buffer(
                                inner_file,
                                nested_paths,
                                archive_mode
                            ).contents)
                    elif zipfile:
                        self.contents.extend(
                            ZipWalker.from_buffer(
                                inner_file,
                                nested_paths
                            ).contents)


# The following classes deal with filtering the results from a file search.
# They're effectively closures over the text or pattern being searched for.

def MatchAll(_):
    return True

class TextSearcher(object):
    """ Simple, non-wildcard search for text in a file name. Can be case-
    sensitive or case-insensitive.
    """
    
    def __init__(self, search_text, case_sensitive):
        """ Creates a callable object that will take a sequence of Path objects
        and search for the given text.
        """
        self.search_text = search_text
        self.case_sensitive = case_sensitive
        
    def __call__(self, paths):
        """ Returns True if the last path in the sequence contains the text. """
        path_tip = paths[-1][-1]
        found = (
            self.case_sensitive and (
                self.search_text.lower() in path_tip.lower()
            )
        ) or (
            self.search_text in path_tip
        )
        return found


class GlobSearcher(object):
    """ UNIX-like glob pattern matching search. Can be case-sensitive or
    case-insenitive.
    """
    
    def __init__(self, pattern, case_sensitive):
        """ Creates a callable object that will take a sequence of Path objects
        and search for those that match the given pattern.
        """
        self.pattern = pattern
        self.case_sensitive = case_sensitive
        
    def __call__(self, paths):
        """ Returns True if the last path in the sequence matches the pattern.
        """
        path_tip = paths[-1][-1]
        found = (
            self.case_sensitive and (
                fnmatchcase(path_tip.lower(), self.pattern.lower())
            )
        ) or (
            fnmatchcase(path_tip, self.pattern)
        )
        return found

# The following deal with presenting the results of a search. They're context
# managers because we might need some sort of opening/closing delimiting (eg.
# for JSON or XML).

@contextmanager
def verbose_reporter():
    """ Format and present a matching path, which could be nested to some
    arbitarily depth, in a way that's easy to visually parse. """
    # We don't actually need a context manager here, so just create the function
    # and yield it.
    def reporter(paths):        
        print paths[0].joined.encode(CLI_ENCODING)
        
        if len(paths) > 1:
            indent_level = 0
            for path in paths[1:]:
                leading = " "*indent_level*len(RESULT_INDENT)
                res_string = "%s%s%s" % (leading, RESULT_INDENT, path.joined)
                print res_string.encode(CLI_ENCODING)
                indent_level += 1
    yield reporter


@contextmanager
def json_reporter():
    """ Prepare a JSON encoder for search results. This needs to be a context
    manager so that the list-of-list of paths can be appropriately delimited.
    """
    contents = deque()
    def report_single_result(paths):
        contents.append([p.joined.encode('utf8') for p in paths])
    yield report_single_result
    json.dump(list(contents), sys.stdout, ensure_ascii=False)
    sys.stdout.write('\n')


# Our script needs a starting point, and that could be a directory OR an
# archive. This dict matches up the MIME type to the appropriate constructor.
SEARCH_BASES = {
    'application/x-bzip2' : ArchiveWalker.from_path,
    'application/x-gzip'  : ArchiveWalker.from_path,
    'application/x-tar'   : ArchiveWalker.from_path,
    'application/zip'     : ZipWalker.from_path,
    'inode/directory'     : DirectoryWalker
}

# If the starting point is neither directory nor archive, it's the only result
DEFAULT_SEARCH_BASE = NamedNode.single

def archive_search(base_path, search_func, reporter):
    """ Recursively list all files and directories below the given base path,
    checking inside directories and archives to an arbitrary depth, and
    filtering using the given filter function.
    
    The base path should be a Path object, and the filter function should take
    a sequence of Path objects and return a boolean if the path is a valid
    result.
    """
    base_path_kind = magic.from_file(base_path.joined, mime=True)
    base_path_class = SEARCH_BASES.get(base_path_kind, DEFAULT_SEARCH_BASE)
    
    # Iterate over a stack of discovered files, filtering as we go
    to_process = deque((base_path_class(base_path),))
    
    with reporter() as report_func:
        while to_process:
            node = to_process.pop()
            paths = node.paths
            
            if search_func(paths):
                report_func(paths)
            
            # If this node has contents, add them to the stack and keep iterating
            to_process.extendleft(node.contents)


def main(base_path, search_text=None, case_sensitive=False, glob=False, json=False):
    """ Glue between the command-line flags and invoking the search function
    with the appropriate filter function.
    """
    if json:
        reporter = json_reporter
    else:
        reporter = verbose_reporter
    
    if search_text is None:
        search_func = MatchAll
    elif glob:
        search_func = GlobSearcher(search_text, case_sensitive)
    else:
        search_func = TextSearcher(search_text, case_sensitive)
    
    archive_search(Path.os(base_path), search_func, reporter)
    

if __name__ == '__main__':
    # Get the incoming CLI arguments into shape for the rest of the script by
    # parsing and decoding them.    
    parser = argparse.ArgumentParser(
        description=PROG_TEXT
    )
    parser.add_argument('search_path')
    parser.add_argument(
        'search_text', nargs='?',
        help="Text or pattern to search for in file names. Omit to list all "
             "files."
    )
    parser.add_argument('-i', '--ignore-case', action='store_true')
    parser.add_argument('-g', '--glob', action='store_true')
    parser.add_argument('-j', '--json', action='store_true')
    
    args = parser.parse_args()
    
    if args.search_text:
        search_text = d_cl(args.search_text)
    else:
        search_text = None
    
    main(
        d_cl(args.search_path),
        search_text,
        case_sensitive=args.ignore_case,
        glob=args.glob,
        json=args.json
        )
